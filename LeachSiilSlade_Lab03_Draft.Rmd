---
title: "Lab 3 - Reducing Crime"
author: "Clayton G. Leach, Karl I. Siil, Timothy S. Slade"
date: "July 23, 2018"
header-includes:
- \usepackage{caption}
- \usepackage{hhline}
- \usepackage{footnote}
- \usepackage{array}
- \usepackage{booktabs}
- \usepackage{xcolor}
- \usepackage{hyperref}
#- \usepackage{minted}
output: 
  pdf_document: 
    latex_engine: xelatex
    #pandoc_args: "--pdf-engine-opt=-shell-escape"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(knitr)
library(here) # TS: Added this to handle the user-agnostic run.
library(car)
library(stargazer)
library(broom)
```

# Introduction

Our client is running for office in the state of North Carolina (NC). Her campaign commissioned us to research the determinants of crime in NC to help her develop her platform regarding crime-related policy initiatives at the level of local government. This report explores a 1994 dataset from Cornwell & Trumball that provides county-level economic, demographic, and crime data. Our analysis describes the dataset, presents initial summary statistics, and develops three linear regression models.

# Initial Exploratory Data Analysis (EDA)

```{r data_import, echo = FALSE, results = "hide"}
#here::dr_here() # TS: Use on first run to see why it fails on your machine, if it does.
here::here() # TS: Use when all set up, so we can mask the output.
#crime_raw <- read_csv('../crime_v2.csv', # TS: Commented out b/c using "here()" don't need ../
crime_raw <- read_csv('crime_v2.csv', col_types = cols( prbconv = col_double()))
problems(crime_raw) # TS: To comment out once we're ready to generate our draft report.
```

## Missing Values

```{r eda_drop_blank_rows}
# KS: Rows with no data
crime_na <- crime_raw %>% filter_all(any_vars(!is.na(.)))
# KS: Row with one back tick
crime_na %>% filter_all(any_vars(is.na(.))) %>% select(which(!is.na(.)))
crime_na <- crime_na %>% filter_all(all_vars(!is.na(.)))
```

Upon loading the data, we examine the `r nrow(crime_raw %>% filter_all(any_vars(is.na(.))))` rows that are missing data, finding that 5 are entirely blank and 1 contains only a backtick. We eliminate those to generate our working dataset.

```{r summary, include=FALSE}
crime_na %>% summary() # TS: I think we'll want to suppress this snippet in the draft we turn in...added include = FALSE
```

## Erroneous Duplicate Records

```{r eda_find_dup_county}
crime_na %>% count(county) %>% filter(n > 1) # county 193 is an exact duplicate 
crime_na %>% filter(county == 193)
```

Continuing our QC, we note that one of the counties' records has been duplicated exactly. We therefore drop the duplicate record from our dataset.

```{r eda_drop_dup_county}
crime_na <- crime_na %>% filter(!duplicated(.))
```

## Plausibility Checks for Variables

Three of our key variables of interest (`prbarr`, `prbconv`, and `prbpris`) represent probabilities and should therefore theoretically be in the range of 0:1.

```{r eda_prob_beyond_range}
# look at weird 'probability' variables.
non_prob <- crime_na %>%
  filter(!between(prbarr, 0, 1) | !between(prbconv, 0, 1) | !between(prbpris, 0, 1))
```

Examining the data, we find `r nrow(non_prob)` counties have values for the "probability" variables that are outside of the expected range. In each case, it is either `prbconv` (10 records) or `prbarr` (1 record) that fall outside the range.

Per the notes accompanying our data, _The probability of conviction is proxied by the ratio of convictions to arrests..._ Given that definition, if not all suspects arrested are convicted, `prbconv` will be below 1. However, it may also exceed 1 if the number of exonerated suspects is exceeded by the number of suspects convicted of multiple charges. (See \href{(https://www.michigancriminallawyer-blog.com/2014/08/this-blog-is-based-upon.html)}{ \textbf{\color{blue}{here}}} for examples of multiple charges stemming from a single arrest.)

The notes on `prbarr` indicate _the probability of arrest is proxied by the ratio of arrests to offenses..._. If multiple suspects are arrested for a single offense, and this happens more frequently than offenses which do not lead to arrests, `prbarr` would indeed exceed 1.

In both cases, there are plausible explanations for the values we observe. Therefore we will not drop these records from our dataset. We will, however, subject them to further scrutiny.

```{r eda_plausibility_checks, echo = FALSE, results = "hide", fig.show = "hide"}
plot(density(crime_na$year)) # TS: Confirmed all 1987
plot(density(crime_na$crmrte)) # TS: Confirmed all positive
plot(density(crime_na$prbarr)) # TS: Confirmed all positive
plot(density(crime_na$prbconv)) # TS: Confirmed all positive
plot(density(crime_na$prbpris)) # TS: Confirmed all positive
plot(density(crime_na$avgsen)) # TS: Confirmed all positive
plot(density(crime_na$polpc)) # TS: Confirmed all positive; in boxplot, 1 outlier
plot(density(crime_na$density)) # TS: Confirmed all positive; in boxplot, 8 outliers
plot(density(crime_na$taxpc)) # TS: Confirmed all positive; in boxplot, several outliers
plot(density(crime_na$west)) # TS: Confirmed all either 0 or 1
plot(density(crime_na$central)) # TS: Confirmed all either 0 or 1
plot(density(crime_na$urban)) # TS: Confirmed all either 0 or 1
plot(density(crime_na$pctmin80)) # TS: Confirmed all positive, between 0 and 100
plot(density(crime_na$wcon))
plot(density(crime_na$wtuc))
plot(density(crime_na$wtrd))
plot(density(crime_na$wfir))
plot(density(crime_na$wser)) # TS: Some serious outliers at > $2000
plot(density(crime_na$wmfg))
plot(density(crime_na$wsta))
plot(density(crime_na$wloc))
plot(density(crime_na$mix))
```

Examining the remainder of our data, we found no substantial evidence of _top-coded_ or _bottom-coded_ (i.e., truncated) variables which might bias our regression models. However, there is an extreme outlier in `wser`, the variable indicating the county's weekly wage in the service industry.  To determine if this is valid we looked at the wage values for other sectors of the economy and did not see elevated values.  It is improbable that individuals in the service industry are making 5-10x more than anyone else in the county, and therefore we will replace this value with an NA. 
<<<<<<< HEAD

--Talk about 1 row with probabiltiy of arrest, conviction, and police as outliers.
=======
>>>>>>> 1de30b0312f3bdb8765fa8da11fa7c8258c8b818

One of the observations, however, appears to be an outlier for some of our variables of interest. The county labeled `115` has the lowest crime rate by far (~50\% lower than that of any other county), the highest `probability' of arrest (>1 arrest per offense, nearly 58% greater than the county with the second-highest probability), the longest average sentence (20.7 days, ~15\% higher than the second-longest), and the largest number of police per capita (9 officers per 1,000 residents, more than twice as many as the second-highest county). While those numbers appear unusual, they are internally consistent: one would expect a very low crime rate from a county that has a very strong police presence, arrests a large proportion of suspects, and punishes convicted criminals severely. \textbf{\color{red}{We will test the robustness of our models by verifying how they change if this observation is retained instead of being treated as an outlier.}}

## Transformation Analysis

If the relationship between two variables is not linear, adding them to a linear regression model as-is (without a transformation) will generate inaccurate results. It is therefore important to explore whether the relationship between two variables is logarithmic, exponential or otherwise and thus whether a transformation is required. As part of our EDA we explored this question for all of the variables in the dataset. For the sake of parsimony we discuss below only those variables which required a transformation.

\[
% The qq-plot of `prbarr` vs. `crmrte` suggests that a transformation may be required to account for variability in the tails. A `log()` transformation appears to reduce some of the variability.
\]

\textbf{\color{red}{--Evaluate the scatter of each of the 5 variables to see if a transform is necessary.
--Final table with new transforms.  
  Police should be log transformed.
  probability of prison should be squared}}

```{r trans_prbarr, out.width="50%", echo = FALSE, results = FALSE}
png("qq_prbarr_base.png")
qq_prbarr_base <- with(crime_na, qqplot(prbarr, crmrte))
dev.off()
png("qq_prbarr_log.png") # Slightly better than base. Transform.
qq_prbarr_log <- with(crime_na, qqplot(log(prbarr), crmrte))
dev.off()
# knitr::include_graphics(c("qq_prbarr_base.png", "qq_prbarr_log.png"))
```
```{r trans_prbconv, out.width="50%", echo = FALSE, results = FALSE}
png("qq_prbconv_base.png")
qq_prbconv_base <- with(crime_na, qqplot(prbconv, crmrte))
dev.off()
# png("qq_prbconv_log.png") # Less good than base. No transform.
# qq_prbconv_log <- with(crime_na, qqplot(log(prbconv), crmrte))
# dev.off()
# knitr::include_graphics(c("qq_prbconv_base.png", "qq_prbconv_log.png"))
```

As can be seen below, the q-q plot of `prbpris` and `crmrte` appears to display some curvature, suggesting a parabola. Transforming the `prbpris` variable by squaring it appears to account for some of that curvature, so we will apply that transformation before including it in our models.

```{r trans_prbpris, echo = FALSE, results = FALSE}
png("qq_prbpris_base.png")
qq_prbpris_base <- with(crime_na, qqplot(prbpris, crmrte),
                main = "Q-Q Plot, Probability of Incarceration vs. Crime Rate",
                xlab = "Probability of Incarceration",
                ylab = "Crime Rate")
dev.off()
# png("qq_prbpris_log.png") # Less good than base. Consider sq
# qq_prbpris_log <- with(crime_na, qqplot(log(prbpris), crmrte))
# dev.off()
png("qq_prbpris_sq.png") # Better than base. Transform.
qq_prbpris_sq <- with(crime_na, qqplot(prbpris^2, crmrte),
                main = "Q-Q Plot, Probability of Incarceration^2 vs. Crime Rate",
                xlab = "Probability of Incarceration^2",
                ylab = "Crime Rate")
dev.off()
```

```{r incl_qqprbpris, out.width="50%"}
knitr::include_graphics(c("qq_prbpris_base.png", "qq_prbpris_sq.png"))
```


```{r trans_avgsen, echo = FALSE, results = FALSE}
png("qq_avgsen_base.png")
qq_avgsen_base <- with(crime_na, qqplot(avgsen, crmrte))
dev.off()
#png("qq_avgsen_log.png") # Doesn't look better. No transformation
#qq_avgsen_log <- with(crime_na, qqplot(log(avgsen), crmrte))
#dev.off()
```

The q-q plot of `polpc` and `crmrte` indicates that the distributions from which they were sampled deviate at higher values of `polpc`. A log transformation appears to improve the fit.

```{r trans_polpc, echo = FALSE, results = FALSE}
png("qq_polpc_base.png")
qq_polpc_base <- with(crime_na, qqplot(polpc, crmrte),
                main = "Q-Q Plot, Police per capita vs. Crime Rate",
                xlab = "Police per capita",
                ylab = "Crime Rate")
dev.off()
png("qq_polpc_log.png") # Better than base. Consider transform
qq_polpc_log <- with(crime_na, qqplot(log(polpc), crmrte,
                main = "Q-Q Plot, log(Police per capita) vs. Crime Rate",
                xlab = "log(Police per capita)",
                ylab = "Crime Rate"))
dev.off()
# png("qq_polpc_sq.png") # Less good than log
# qq_polpc_sq <- with(crime_na, qqplot(polpc^2, crmrte))
# dev.off()
# png("qq_polpc_sqrt.png") # Less good than log
# qq_polpc_sqrt <- with(crime_na, qqplot(sqrt(polpc), crmrte))
# dev.off()
# png("qq_polpc_log.png") # Not as good as log(polpc)
# qq_polpc_log-log <- with(crime_na, qqplot(log(polpc), log(crmrte)))
# dev.off()
```

```{r incl_polpc, out.width="50%"}
knitr::include_graphics(c("qq_polpc_base.png", "qq_polpc_log.png"))
```

The q-q plot of tax revenue per capita and crime rate appears to be highly affected by the presence of an outlier. Applying a log transformation to the tax revenue variable improves the fit without requiring the outlier to be dropped. \textbf{\color{red}{Once we begin constructing models we will verify whether they are robust to retaining this outlier. [TS: <- To-do during cleanup / QC...]}}

```{r trans_taxpc, echo = FALSE, results = FALSE}
png("qq_taxpc_base.png")
qq_taxpc_base <- with(crime_na, qqplot(taxpc, crmrte),
                main = "Q-Q Plot, Tax Revenue per capita vs. Crime Rate",
                xlab = "Ta Revenue per capita",
                ylab = "Crime Rate")
dev.off()
png("qq_taxpc_log.png") # Better than base. Consider transform
qq_taxpc_log <- with(crime_na, qqplot(log(taxpc), crmrte,
                main = "Q-Q Plot, log(Tax Revenue per capita) vs. Crime Rate",
                xlab = "log(Tax Revenue per capita)",
                ylab = "Crime Rate"))
dev.off()
```

```{r incl_taxpc, out.width="50%"}
knitr::include_graphics(c("qq_taxpc_base.png", "qq_taxpc_log.png"))
```

\textbf{\color{red}{Discuss percent minority transformation here...GUYS: this is obviously the wrong transformation, but it's here as a placeholder. Take a look at the qq plot down at the end of the document. pctmin80 seems to have a clear sigmoid shape...what would be the right transformation there? Both log and square have been bad, so I'm thinking maybe we just don't transform at all? See below...}}

```{r trans_pctmin80, echo = FALSE, results = FALSE}
png("qq_pctmin80_base.png")
qq_pctmin80_base <- with(crime_na, qqplot(pctmin80, crmrte),
                main = "Q-Q Plot, Percent Minority in 1980 vs. Crime Rate",
                xlab = "Percent Minority in 1980",
                ylab = "Crime Rate")
dev.off()
png("qq_pctmin80_log.png") # Worse than base. Wrong transform
qq_pctmin80_log <- with(crime_na, qqplot(log(pctmin80), crmrte,
                main = "Q-Q Plot, Percent Minority in 1980 vs. Crime Rate",
                xlab = "log(Percent Minority in 1980)",
                ylab = "Crime Rate"))
dev.off()
png("qq_pctmin80_sq.png") # Better than base. Consider transform
qq_pctmin80_sq <- with(crime_na, qqplot(pctmin80^2, crmrte,
                main = "Q-Q Plot, (Percent Minority in 1980)^2 vs. Crime Rate",
                xlab = "(Percent Minority in 1980)^2",
                ylab = "Crime Rate"))
dev.off()
```

```{r incl_pctmin80, out.width="50%"}
knitr::include_graphics(c("qq_pctmin80_base.png", "qq_pctmin80_log.png"))
knitr::include_graphics(c("qq_pctmin80_base.png", "qq_pctmin80_sq.png"))
```

The q-q plot of population density (persons per square mile) and crime rate appears to be an ill fit. Transforming the population density by taking the square appears to correct the misalignment, albeit with an outlier that appears to be very rural and have a low crime rate.

\textbf{\color{red}{Guys: I'm not sure what the square root of persons per square mile means. Is this one where we forego the transformation because of the lack of interpretability? Also, I think the untransformed version seems to have more "outliers" than the square root version, but I'm def. not married to it.}}

```{r trans_density, echo = FALSE, results = FALSE}
png("qq_density_base.png")
qq_density_base <- with(crime_na, qqplot(density, crmrte),
                main = "Q-Q Plot, Percent Minority in 1980 vs. Crime Rate",
                xlab = "Percent Minority in 1980",
                ylab = "Crime Rate")
dev.off()
png("qq_density_sqrt.png") # Better than base. Consider transform
qq_density_sqrt <- with(crime_na, qqplot(sqrt(density), crmrte,
                main = "Q-Q Plot, Square Root of People per Square Mile vs. Crime Rate",
                xlab = "Square Root of People per Square Mile",
                ylab = "Crime Rate"))
dev.off()
```

```{r incl_density, out.width="50%"}
knitr::include_graphics(c("qq_density_base.png", "qq_density_sqrt.png"))
```

```{r trans_pctymle, echo = FALSE, results = FALSE}
png("qq_pctymle_base.png")
qq_pctymle_base <- with(crime_na, qqplot(pctymle, crmrte),
                main = "Q-Q Plot, Percent Young Males vs. Crime Rate",
                xlab = "Percent Young Males",
                ylab = "Crime Rate")
dev.off()
png("qq_pctymle_log.png") # Better than base. But looks like it needs more
qq_pctymle_log <- with(crime_na, qqplot(log(pctymle), crmrte,
                main = "Q-Q Plot, Percent Young Male vs. Crime Rate",
                xlab = "log(Percent Young Male)",
                ylab = "Crime Rate"))
dev.off()
```

The q-q plot of the percentage of young males and the crime rate appears to reveal a non-linear relationship. Applying a log transformation to the percent young male variable appears to improve the fit.

```{r incl_pctymle, out.width="50%"}
knitr::include_graphics(c("qq_pctymle_base.png", "qq_pctymle_log.png"))
```

## Variables Available for Analysis

The table below details the variables available to us, which model(s) we included them in, and any transformations we applied before including them in our model.

\textbf{\color{red}{To do: remove the 'used in models...' column, just add that information in the discussion before each model}}

\begin{center} % https://community.rstudio.com/t/inserting-table-in-r-markdown/7260/4
\captionof{table}{Hypothesized Primary Determinants of Observed Crime Rate}
\begin{tabular}{|l|l|p{2.5cm}|p{2cm}|}
\hline
Variable Name & Description & Transformation Applied & Models Using \\ \hhline{|=|=|=|=|}
\texttt{county} & \textit{Source county of data} & - & - \\
\texttt{year} & \textit{Source year of data} & - & - \\
\texttt{crmrte} & \textit{crime rate} & - & 1, 2, 3, 4, 5 \\
\texttt{prbarr} & \textit{`probability' of arrest} & - & 1, 2, 3, 4, 5 \\
\texttt{prbconv} & \textit{`probability' of conviction} & - & 1, 2, 3, 4, 5 \\
\texttt{prbpris} & \textit{`probability' of prison sentence} & square ($prbpris^2$) & 1, 2, 3 \\
\texttt{avgsen} & \textit{average sentence, in days} & - & 2, 3 \\
\texttt{polpc} & \textit{police per capita} & $\log(polpc)$ & 1, 2, 3, 4, 5 \\
\texttt{density} & \textit{people per sq. mile} & \color{red}{$\sqrt(density)$} & 2, 3, 4 \\
\texttt{taxpc} & \textit{tax revenue per capita} & $\log(taxpc)$ & 2, 3 \\
\texttt{west} & \textit{Dummy: source county of data is in Western NC} & - & 3 \\
\texttt{central} & \textit{Dummy: source county of data is in Central NC} & - & 3 \\
\texttt{urban} & \textit{Dummy: source county of data is urban} & - & 3 \\
\texttt{pctmin80} & \textit{percent minority in 1980} & \color{red}{TBD-Guys??} & 2, 3, 4 \\
\texttt{wcon} & \textit{wages in the construction industry} & - & 3 \\
\texttt{wtuc} & \textit{wages in the transportation, utilities, and communication industries} & - & 3 \\
\texttt{wtrd} & \textit{wages in the construction industry} & - & 3 \\
\texttt{wfir} & \textit{wages in the finance, insurance, real estate industries} & - & 3 \\
\texttt{wser} & \textit{wages in the service industry} & - & 3 \\
\texttt{wmfg} & \textit{wages in the manufacturing industry} & - & 3 \\
\texttt{wfed} & \textit{wages among federal employees} & - & 3 \\
\texttt{wsta} & \textit{wages among state employees} & - & 3 \\
\texttt{wloc} & \textit{wages among local government employees} & - & 3 \\
\texttt{mix} & \textit{mix of offenses; face-to-face v others} & - & 3 \\
\texttt{pctymle} & \textit{percent young male} & $\log(pctymle)$ & 2, 3 \\
\hline
\end{tabular}
\end{center}

\[
%\begin{table}[!h]
%\centering
%\caption{Hypothesized Primary Determinants of Observed Crime Rate}
%\label{pntable}
%%\begin{tabular}{l>{\raggedright\arraybackslash$}p{2cm}>{$}|l|c}
%\begin{tabular}{p{1.5cm} | >{\raggedright}p{3cm} | p{8cm} | p{2cm}}
%\toprule
%\centering\textbf{Variable Name} & \centering\textbf{Explanation} & \centering\textbf{Reasoning} & \centering\textbf{Transformation Applied} & \\
%\midrule
%\texttt{polpc} & \textit{police per capita} & Police may act as a deterrent to crime, may increase the observed crime rate, or both. & - \\
%\texttt{pctymle} & \textit{percent young male} & Young males commit and are charged with a disproportionate share of crimes & - \\
%\texttt{density} & \textit{people per sq. mile} & Greater population density increases opportunity for crimes to be committed and reported & - \\
%\texttt{taxpc} & \textit{tax revenue per capita} & Lower tax revenues may be associated with poorer community-government relations, greater economic hardship, and less policing \footnote{This may introduce collinearity with several other variables} & $log_{10}$  \\
%\texttt{prbarr} & \textit{`probability' of arrest} & Greater probability of arrest may serve a deterrent function & - \\
%\texttt{prbconv} & \textit{`probability' of conviction} & Greater probability of conviction may serve a deterrent function & - \\
%\texttt{prbpris} & \textit{`probability' of prison sentence} & Greater probability of sentencing may serve a deterrent function & - \\
%\texttt{avgsen} & \textit{average sentence, in days} & Harsher sentencing practices may serve a deterrent function & - \\
%\texttt{pctmin80} & \textit{percent minority in 1980} & Minorities are disproportionately arrested and convicted of crimes & - \\
%\bottomrule
%\end{tabular}
%\end{table}
\]

# Research Question and Model-Building

Our **research question** is the following: \textbf{\color{red}{Should our candidate support a traditional "Tough on Crime" platform?}}

We face a key limitation: our data does not give us visibility into crime, it only gives us insight into the official _crime rate_. The crime rate is a function not only of crimes committed but also of various factors, some of which may be unobservable. For instance, poor community-police relations may bias crime rates downward if an area's residents \href{https://www.washingtonpost.com/news/acts-of-faith/wp/2018/04/19/churches-make-a-drastic-pledge-in-the-name-of-social-justice-to-stop-calling-the-police/?utm_term=.ac2ea8ee1523}{\textbf{\color{blue}{do not report all the crimes they observe or experience}}}. Conversely, those poor relations may also bias crime rates upward if police officers engage in \href{https://www.huffingtonpost.com/2015/05/04/st-louis-county-predatory-policing_n_7201964.html}{\textbf{\color{blue}{predatory policing practices}}} and the community lacks the wherewithal to fight back. As we report our findings we will make note of potential bias that results from our inability to observe and analyze critical variables.

In order to answer our research question we created a model which included variables related to key crime policy decisions.  We only included variables which would allow our canadidate to make concrete policy proposals that lie within her purview.

Using the aforementioned criteria we choose the variables police per capita (`polpc`), probability of arrest (`prbarr`), probability of conviction (`prbconv`), probability of incarceration (`prbpris`), and average sentence length (`avgsen`) for our first model.  Understanding how these items relate to crime rate can help shape her position, and understand whether a "Tough on Crime" stance does in fact acheive a reduction in crime.  Specifically, we can help her understand which departement/function should receive additional funding given a limited budget (e.g. if conviction rates are highly correlated perhaps we invest more in our District Attorneys).

\begin{center} % https://community.rstudio.com/t/inserting-table-in-r-markdown/7260/4
\captionof{table}{Model 1: Hypothesized Key Determinants of Observed Crime Rate}
\begin{tabular}{|l|l|l|}
\hline
Variable Name & Description & Transformation Applied \\ \hhline{|=|=|=|}
\texttt{prbarr} & \textit{`probability' of arrest} & - \\
\texttt{prbconv} & \textit{`probability' of conviction} & - \\
\texttt{prbpris} & \textit{`probability' of prison sentence} & square ($prbpris^2$) \\
\texttt{avgsen} & \textit{average sentence, in days} & - \\
\texttt{polpc} & \textit{police per capita} & $\log(polpc)$ \\
\hline
\end{tabular}
\end{center}

<<<<<<< HEAD

## Transformation Analysis

--Evaluate the scatter of each of the 5 variables to see if a transform is necessary.
--Final table with new transforms.  
  Police should be log transformed.
  probability of prison should be squared

``` {r mod1_notransf, echo = FALSE, results = FALSE}

=======
``` {r mod1_notransf, echo = FALSE, results = FALSE}
>>>>>>> 1de30b0312f3bdb8765fa8da11fa7c8258c8b818
#Linear Regression model using only our key variables of interest.
model1 <- with(crime_na, lm(crmrte ~ polpc+prbarr+prbconv+prbpris+avgsen))

#Adding AIC to our model to help us compare models in the future.
model1$AIC <- AIC(model1)

#Output model results in nice format using tidy and kable
kable(tidy(model1) %>% select(-std.error))
```

``` {r mod1_transforms}

#Linear Regression model using only our key variables of interest, transformed as needed
model1_trans <- with(crime_na, lm(crmrte ~ log(polpc) + log(prbarr) +
                                    prbconv + prbpris^2 + avgsen))

#Adding AIC to our model to help us compare models in the future.
model1_trans$AIC <- AIC(model1_trans)

#Output model results in nice format using tidy and kable
kable(tidy(model1_trans) %>% select(-std.error),
      caption = 'Model 1: Hypothesized Key Determinants of Observed Crime Rate')
```

**Comments on Model 1:**

The log of police per capita, the log of the probability of arrest, and the probability of conviction all have high levels of significance. The overall model has an adjusted $R^2$ of .5232.  There are three main points to highlight:

  1) Our coefficient for police per capita (`polpc`) is positive, large, and highly significant. If we inaccurately assumed this model was causal the best mechanism to reduce crime rates would be to sunset the police force!  This model is not causal, however, and a more plausible interpretation is that a higher number of police per capita is a response to higher levels of criminal activity. 
  
  2) Both probability of arrest (`prbarr`) and probability of conviction (`prbconv`) have highly significant and negative coefficients.  This fits with what we would expect: the more likely an individual is to be caught and convicted the less likely they are to commit crime.
  
  3) Neither the square of the probability of incarceration (`prbpris`) nor average sentence length (`avgsen`) were statistically significant.  Furthermore, the coefficient for the squared probability of incarceration rate is positive, which is counterintuitive. 

## Model 2

While our first model showed promise, there are several other factors which might be correlated with these explanatory variables; this would lead to multicollinearity issues.  When multiple independent variables exhibit collinearity it becomes increasingly difficult to untangle their individual effects on the crime rate.  This is troublesome because the model coefficients  may misrepresent the impact of a variable, leading to a policy which has underwhelming impact.  In order to control for this we created a second model which includes variables we believe to be highly correlated with our three key variables.

\textbf{Collinearity with Police Per Capita}: We would expect policing practices in urban areas to differ substantially from those in suburban or rural areas; including density and the urban dummy variable can help control for this. Additionally, the police force is funded by taxpayers, and therefore we might expect a larger police force per capita to be correlated with higher tax revenues.  \textit{\color{red}{Given the assumed diminishing marginal returns of money}} \textbf{\color{red}{[TS: <- Could you unpack that a bit more, Clay?]}} it makes sense to include the log transformation so that we can interpret our coefficient as the change given a 1% increase.

### Creation of variables to proxy tax revenue

Our dataset contains 9 wage variables, each representing a different sector or group of industries.  We do not have \textit{a priori} justification to believe \textit{a single industry} \textbf{\color{red}{[TS: <- This feels a bit awk; to wordsmith...]}} might contribute disproportionately to crime, but we can assume that low wages in general might create an environment of economic scarcity in which crime incidence would increases.  Including all 9 variables when our dataset only contains 90 observations would be extremely limiting, \textbf{\color{red}{[TS: <- Poss. to add a phrase explaining why?]}} but excluding them entirely prohibits us from understanding how microeconomic conditions may be contributing to observed crime rates.  Researching the composition of each county's economy and weighting each variable accordingly might be a fruitful strategy, but it lies outside the scope of this report.  The solution we ultimately implemented was to create three new composite variables:

  1) Government Wage: Average of `wfed` (federal government wage), `wsta` (state government wage), and `wloc` (local government wage)
  2) Blue-Collar \textbf{\color{red}{[TS: <- Okay edit? Service industry not really physical labor in same way as manufacturing and construction...]}} Wage: Average of `wmfg` (manufacturing), `wser` (service), `wcon` (construction)
  3) Professional Wage: Average of `wfir` (Finance/Investment/Real Estate), `wtrd` (Wholesale/Retail Trade), and `wtuc` (Transportation, Utilities, Communication) \textbf{\color{red}{[TS: <- I'd advocate for grouping `wtuc` with blue-collar; maybe retail as well. It's likely generate incomes closer to the blue-collar range than the professional wage. That would leave wfir alone, so no need for a composite var...]}}

```{r}
crime_na %>% 
  gather(wfed, wtuc, wsta, wmfg, wfir, wloc, wcon, wser, wtrd, key = 'work_type',
         value = 'weekly_wage') %>%
  select(county, work_type, weekly_wage, everything()) %>%
  mutate(work_type = factor(work_type, c('wfed', 'wtuc', 'wsta', 'wmfg', 'wfir',
                                         'wloc', 'wcon', 'wser', 'wtrd'))) %>%
  ggplot(aes(x = weekly_wage, fill = work_type)) +
  geom_density(alpha = 0.5) +
  scale_fill_brewer(palette = 'Spectral') +
  ggtitle('Weekly Wage Density by position') +
  xlab('Weekly Wage $/week') +
  ylab('Density') +
  labs(fill = 'Work type') +
  xlim(0, 750) 
```

``` {r new_variable_creation}
crime_na$govt_wg <- (crime_na$wfed+crime_na$wsta+crime_na$wloc)/3
crime_na$physical_wg <- (crime_na$wmfg+crime_na$wser+crime_na$wcon)/3
crime_na$industry_wg <- (crime_na$wfir+crime_na$wtrd+crime_na$wtuc)/3
```

\textbf{Collinearity with Probability of Arrest}: Sociological research suggests that men - especially young, minority men - are at an increased risk of arrest.  Therefore we will include both the `pctymle` (percentage of young male, under a log transformation) and `pctmin80` (percentage of minorities in 1980) variables. 

\textbf{Collinearity with Probability of Conviction}: The likelihood of an arrest leading to a conviction depends not only on the culpability of the suspect, but also on the quality or effectiveness of the police department's investigative team, the district attorney, court-appointed advocates, judges, and other government officials.  Quality is an unobservable (omitted) variable, but it  might be loosely correlated with the government wage. We will thus include our government wage variable as a proxy.

\textbf{Collinearity with Probability of Incarceration and Average Sentencing}: While there may be unobservable covariates for these two variables, we cannot identify any reasonable proxies to include in a revised model to address the issue.

\begin{center} % https://community.rstudio.com/t/inserting-table-in-r-markdown/7260/4
\captionof{table}{Model 2: Hypothesized Key Determinants of Observed Crime Rate with Additional Covariates}
\begin{tabular}{|l|l|l|}
\hline
Variable Name & Description & Transformation Applied \\ \hhline{|=|=|=|}
\texttt{prbarr} & \textit{`probability' of arrest} & - \\
\texttt{prbconv} & \textit{`probability' of conviction} & - \\
\texttt{prbpris} & \textit{`probability' of prison sentence} & square ($prbpris^2$) \\
\texttt{polpc} & \textit{police per capita} & $\log(polpc)$ \\
\texttt{avgsen} & \textit{average sentence, in days} & - \\
\texttt{taxpc} & \textit{tax revenue per capita} & $\log(taxpc)$ \\
\texttt{density} & \textit{persons per square mile} & $\sqrt{density}$ \\
%\texttt{govt_wg} & \textit{mean wage across government sectors} & - \\
\texttt{pctymle} & \textit{percentage of young males} & $\log(pctymle)$ \\
\hline
\end{tabular}
\end{center}

``` {r mod_2notransf, echo = FALSE, results = FALSE}
#Model with our key explanatory variables, and what we suspect to be key covariates
model2 <- with(crime_na, lm(crmrte ~ polpc + prbarr + prbconv +
                            prbpris + avgsen + taxpc + density +
                            govt_wg + pctymle + pctmin80))

model2_no_minorityvariable <- with(crime_na, lm(crmrte ~ polpc + prbarr + prbconv + 
                                                prbpris + avgsen + taxpc + 
                                                density + govt_wg + pctymle))

added_adj_r_squared <- summary(model2)$adj.r.squared - summary(model2_no_minorityvariable)$adj.r.squared

#Adding AIC to our model to help us compare models in the future.
model2$AIC <- AIC(model2)

#Output model results in nice format using tidy and kable
kable(tidy(model2))
```

``` {r mod_2transforms}
#Model with our key explanatory variables, and what we suspect to be key covariates
model2_trans <- with(crime_na, lm(crmrte ~ log(polpc) + prbarr + 
                                  prbconv + prbpris + avgsen + log(taxpc) + sqrt(density) + 
                                  govt_wg + log(pctymle) + pctmin80))

model2_trans_no_minvar <- with(crime_na, lm(crmrte ~ polpc + prbarr +
                                            prbconv + prbpris + avgsen + log(taxpc) +
                                            sqrt(density) + govt_wg + log(pctymle)))

added_adj_r_squared <- summary(model2_trans)$adj.r.squared - summary(model2_trans_no_minvar)$adj.r.squared

#Adding AIC to our model to help us compare models in the future.
model2_trans$AIC <- AIC(model2_trans)

#Output model results in nice format using tidy and kable
kable(tidy(model2_trans))
```

\textbf{Comments on Model 2}:

1) One item of interest was the extreme degree of significance we see associated with our percent minority variable (`pctmin80`).  Interestingly, when using only this variable to predict crime rates our $R^2$ is very low; after controlling for other factors, however, this variable becomes extremely important. One way to measure this importance is by calculating the difference between the adjusted $R^2$ values for a model that includes the variable and one that excludes it. When including the minority variable in our model, the adjusted $R^2$ is `r round(summary(model2_trans)$adj.r.squared,3)`. When excluding it, the adjusted $R^2$ is `r round(summary(model2_trans_no_minvar)$adj.r.squared,3)`, a difference of `r round((summary(model2_trans)$adj.r.squared-summary(model2_trans_no_minvar)$adj.r.squared),3)`

``` {r minority_variable_addition, echo = FALSE, results = FALSE}
# TS: Just did this in-line, above...could omit this chunk, unless we want to keep it to be transparent about how we derived the figures...
cat("When including the minority variable our adjusted R squared is,",round(summary(model2)$adj.r.squared,3),"and when excluding this variable our adjusted R squared is,",round(summary(model2_no_minorityvariable)$adj.r.squared,3),",which equates to a delta of",round((summary(model2)$adj.r.squared-summary(model2_no_minorityvariable)$adj.r.squared),3))
```

2) The square root of the population density is also highly significant. However, it is unclear what the interpretation may be. Absent the square root transformation, the coefficient is ~.0055, indicating that an additional person per square mile increases the crime rate by .55%. \textbf{\color{red}{TS: Is that right? I'm not so sure about it...}}

## Model 3

Given the countless ways behavioral issues are interconnected, we wondered whether every variable we had data on might be correlated with either the crime rate or an already included variable in some fashion.  Our focus was to determine if including all our variables substantially changed the significance or corefficient of any of our previously included variables.  Additionally, we wanted to understand if any of the variables we had previously left out were in fact predictive overall. 

``` {r mod_3notransf, echo = FALSE, results = FALSE}
#Linear model including our key explanatory variables, suspected covariates, and most other variables
model3 <- with(crime_na, lm(crmrte ~ polpc + prbarr + prbconv + prbpris +
                            avgsen + taxpc + density + govt_wg + pctymle + pctmin80 +
                            west + central + urban + physical_wg + industry_wg + mix))

#Adding AIC to our model to help us compare models in the future.
model3$AIC <- AIC(model3)

#Output model results in nice format using tidy and kable
kable(tidy(model3))
```

``` {r mod_3transforms}
#Linear model including our key explanatory variables, suspected covariates, and most other variables
model3_trans <- with(crime_na, lm(crmrte ~ log(polpc) + prbarr + prbconv + prbpris^2 +
                            avgsen + taxpc + sqrt(density) + govt_wg + log(pctymle) +
                            pctmin80 + west + central + urban +
                            physical_wg + industry_wg + mix))

#Adding AIC to our model to help us compare models in the future.
model3_trans$AIC <- AIC(model3_trans)

#Output model results in nice format using tidy and kable
kable(tidy(model3_trans))
```

Model 3 Notes:

## Model Comparison

Below is a comparison table for our three models, along with key statistics related to the model.  Our findings match with what we would hope to see from a model building perspective: Our AIC and adjusted R squared numbers suggest that the model which includes both our key explanatory variables and plausible covariates performs the best.  Our model which excludes key covariates has significantly less predictive power (as measured by adjusted R squared), and our model which includes everything--despite having the highest unadjusted R squared--performed worse as we arbitrarily added additional variables.

```{r stargazer, results = "asis"}
# Code from here: https://stackoverflow.com/questions/47494761/show-akaike-criteria-in-stargazer (using the example 2 approach)
stargazer(model1, model2, model3,
          type = "latex", report="vc", header=FALSE,
          title = "Linear Models Predicting Crime Rate",
          keep.stat = c("aic", "rsq","adj.rsq","n"), omit.table.layout = "n")


```

## Omitted Variables

Despite the promising results from our three models it is difficult to ascribe causality to the variables of interest.  One issue with causal inference in general is ommitted variable bias, which can invalidate our ability to assume each explanatory variable is uncorrelated with the error term.  While there are infinite variables which exist, there are several which deserve commentary:


1) Political Party in Control: Traditionally the issue of crime policy is a highly partisan issue, with each party having very different approaches to crime reduction.  All else being equal, we might expect police levels and average sentence lengths to be correlated with the party that is crafting legislation.  Assuming we construct our variable as a boolean indicator ("is_republican"), we might expect the coefficient for police per capita to diminish as we assume a priori that higher police levels are correlated with conservative crime policies.  We could include this data by appending public records to our dataset which would be more appropriate than trying to find some other variable to proxy.

2) Unemployment Rate: While we have data on weekly wages, this does nothing to tell us what percentage of the population was actually earning those wages.  It is likely that a higher unemployment rate would be correlated with higher rates of crime as people who may not normally commit criminal activity are pushed to their limits.  We might also wish to be more granular, and include both minority and majority unemployment rates to help control for racial inequality.  We realistically could obtain this data from the Bearue of Labor Statistics and append it to our dataset; we leave this next step to future researchers.

3) Concentrated/Siloed Urban Blight: Our data is at the county level and therefore may obscure differences within the county.  We would expect there to be a difference in crime rates between a county which is relatively homogenous with respect to the variables, and one which has drastic differences (e.g. a very poor area and a very nice area).  One way to proxy this might be to calculate a normalized standard deviation of housing prices which could help capture if this phenomenom exists.  

4) Policing Methodology:

5) Police Representation: 

<<<<<<< HEAD

6)

=======
>>>>>>> 1de30b0312f3bdb8765fa8da11fa7c8258c8b818
## Findings and Policy Recommendations

## Conclusion

```{r}
mod1vars <- c("polpc","prbarr","prbconv","prbpris","avgsen")
mod2vars <- c("taxpc","density","govt_wg","pctymle","pctmin80")
mod3vars <- c("")
for (v in mod1vars){
  print(ggplot(crime_na, aes(y = crmrte)) +
          geom_point(aes_string(x = v)) + 
          xlab(v) +
          ylab('Crime Rate') +
          ggtitle(str_glue('Crime Rate vs {v}'))
          )
  print(ggplot(crime_na) +
          geom_qq(aes_string(sample = v)) +
            xlab(v))
}
for (v in mod2vars){
  print(ggplot(crime_na, aes(y = crmrte)) +
          geom_point(aes_string(x = v)) + 
          xlab(v) +
          ylab('Crime Rate') +
          ggtitle(str_glue('Crime Rate vs {v}'))
          )
  print(ggplot(crime_na) +
          geom_qq(aes_string(sample = v)) +
            xlab(v))
}
```

```{r make_scatter}
make_scatters <- function(df, var_list, y, trans) {
  
  if(!missing(trans)) {
    var_list <- append(var_list, str_glue('{trans}({var_list})'))
  }

  for (v in var_list){
  print(ggplot(df, aes_string(x = v, y = y)) +
          geom_point() + 
          geom_smooth(method = 'lm', se = FALSE) +
          xlab(v) +
          ylab(y) +
          ggtitle(str_glue('{y} vs {v}'))
        )
    } 
}


make_scatters(crime_na, mod1vars, y = 'crmrte')


```


``` {r mod_4transforms}
#Linear model retaining only significant variables from mod_2
model_trans <- with(crime_na, lm(crmrte ~ log(polpc) + prbarr + prbconv + prbpris^2 +
                            avgsen + taxpc + sqrt(density) + govt_wg + log(pctymle) +
                            pctmin80 + west + central + urban +
                            physical_wg + industry_wg + mix))

#Adding AIC to our model to help us compare models in the future.
model3_trans$AIC <- AIC(model3_trans)

#Output model results in nice format using tidy and kable
kable(tidy(model3_trans) %>% select(-std.error))
```